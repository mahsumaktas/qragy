<p align="center">
  <img src="public/qragy_logo.jpg" alt="Qragy Logo" width="180">
</p>

<h1 align="center">Qragy</h1>

<p align="center">
  <strong>Self-hosted RAG chatbot that runs on a Raspberry Pi. $0/month.</strong>
</p>

<p align="center">
  <a href="https://github.com/mahsumaktas/qragy/stargazers"><img src="https://img.shields.io/github/stars/mahsumaktas/qragy?style=flat&color=f5a623" alt="GitHub Stars"></a>
  <a href="https://github.com/mahsumaktas/qragy/releases"><img src="https://img.shields.io/github/v/release/mahsumaktas/qragy?color=blue" alt="Release"></a>
  <a href="LICENSE"><img src="https://img.shields.io/badge/license-MIT-blue.svg" alt="MIT License"></a>
  <img src="https://img.shields.io/badge/node-18%2B-brightgreen.svg" alt="Node 18+">
  <img src="https://img.shields.io/badge/JavaScript-ES2022-F7DF1E?logo=javascript&logoColor=black" alt="JavaScript">
  <a href="CONTRIBUTING.md"><img src="https://img.shields.io/badge/PRs-welcome-brightgreen.svg" alt="PRs Welcome"></a>
</p>

<p align="center">
  <a href="#quick-start">Quick Start</a> Â·
  <a href="#features">Features</a> Â·
  <a href="#architecture">Architecture</a> Â·
  <a href="#admin-panel">Admin Panel</a> Â·
  <a href="#deploy">Deploy</a> Â·
  <a href="#api">API</a> Â·
  <a href="#configuration">Configuration</a>
</p>

<p align="center">
  <a href="https://render.com/deploy?repo=https://github.com/mahsumaktas/qragy"><img src="https://render.com/images/deploy-to-render-button.svg" alt="Deploy to Render"></a>
</p>

---

## Why Qragy?

**Dify needs Docker, Redis, and Postgres.** Botpress is cloud-only. Intercom costs $74/seat/month.

**Qragy needs one command: `npm start`.** Multi-model support (Gemini, OpenAI, Ollama) with zero extra dependencies.

It uses [LanceDB](https://lancedb.com) (embedded vector DB) and supports multiple AI providers, so you get a production-ready AI support chatbot with zero infrastructure cost â€” even on a **$35 Raspberry Pi**.

> One process. One CSV file. 7 npm dependencies. Zero cloud bills.

| Feature | Qragy | Dify | LibreChat | Open WebUI |
|---------|-------|------|-----------|------------|
| Runs on Pi | Yes | No | No | No |
| Min RAM | 150MB | 4GB+ | 2GB+ | 1GB+ |
| Dependencies | 7 npm | Docker+ | 5 services | Docker+ |
| Setup time | 30sec | 30min+ | 20min+ | 15min+ |
| Multi-model | Yes | Yes | Yes | Yes |
| RAG built-in | Yes | Yes | No | Yes |
| Admin panel | Yes | Yes | No | Yes |

---

## Features

### ğŸ§  RAG-Powered AI
- **Vector search** over your knowledge base using LanceDB (embedded, serverless)
- **Multi-model** â€” Gemini, OpenAI, Ollama (local LLM) out of the box
- **Topic routing** â€” keywords + AI classify issues into structured flows
- **Deterministic collection** â€” bot gathers required info before escalating
- **Model fallback** â€” automatic retry with a secondary model
- **Smart chunking** â€” markdown, recursive, and sentence-based document splitting

### ğŸ›ï¸ Admin Panel (`/admin`)
Manage everything from the browser â€” no code, no CLI:

| Tab | What You Can Do |
|-----|----------------|
| **Tickets** | Full chat histories, handoff status, assignment, priority, internal notes |
| **Knowledge Base** | CRUD for Q&A entries, file upload (PDF/DOCX/TXT), one-click re-embed |
| **Bot Config** | Edit persona, topics, escalation rules, memory templates, env vars |
| **Analytics** | Daily metrics, top topics, resolution rates, SVG charts |
| **System** | Health monitoring, uptime, memory usage, hot-reload |

### ğŸ“¦ Zero Infrastructure
- **LanceDB** â€” embedded vector DB, no separate server (unlike Pinecone/Weaviate/Qdrant)
- **File-based storage** â€” CSV + JSON + LanceDB files. No PostgreSQL, no Redis
- **Single process** â€” one `node server.js`, that's it
- **No build step** â€” vanilla JS frontend, zero bundling

### ğŸ”Œ Integrations
- **Zendesk** â€” automatic widget + Sunshine Conversations handoff
- **Telegram** â€” bot channel via long polling
- **Webhooks** â€” HMAC-SHA256 signed events to Slack, n8n, Zapier
- **Embeddable widget** â€” one `<script>` tag on any website

### ğŸ†“ Free Embedding Models

| Provider | Model | Dimensions | Cost |
|----------|-------|-----------|------|
| **Google Gemini** *(default)* | `gemini-embedding-001` | 3072 | Free tier |
| **OpenAI** | `text-embedding-3-small` | 1536 | $0.02/1M tokens |
| **Ollama** | `nomic-embed-text` | 768 | Free (local) |

### ğŸš€ v2 Highlights
- **Multi-model support** â€” Gemini + OpenAI + Ollama via raw fetch() (zero new deps)
- **Document chunking engine** â€” markdown, recursive, sentence strategies
- **Docker support** â€” `docker run` and you're live
- Rate limiting (per-IP, configurable)
- File upload with auto-chunking (PDF, DOCX, TXT)
- Team features: ticket assignment, priority levels, internal notes
- Prompt versioning with auto-snapshot and rollback
- Auto-deploy webhook support

---

## Architecture

```mermaid
graph TB
    subgraph Client
        CW[Chat Widget]
        TG[Telegram Bot]
        EMB[Embed Script]
    end

    subgraph "Qragy Server (single process)"
        EXP[Express.js API]
        TC[Topic Classifier]
        RAG[RAG Engine]
        TKT[Ticket System]
        ADM[Admin Panel]
        WH[Webhook Dispatcher]
    end

    subgraph Storage["Local Storage (no external DB)"]
        LDB[(LanceDB<br/>Vector Index)]
        CSV[(CSV + JSON<br/>Knowledge Base)]
        TDB[(Tickets DB<br/>JSON)]
    end

    subgraph External
        LLM[LLM Provider<br/>Gemini / OpenAI / Ollama]
        ZD[Zendesk<br/>Handoff]
    end

    CW & TG & EMB -->|HTTP / Long Poll| EXP
    EXP --> TC -->|topic match| RAG
    RAG -->|vector search| LDB
    RAG -->|read/write| CSV
    RAG -->|generate| LLM
    EXP --> TKT --> TDB
    TKT -->|escalate| ZD
    TKT -->|notify| WH
    EXP --> ADM
```

**Message flow:**

1. User sends a message â†’ **Topic detection** (keywords + AI classification)
2. **RAG search** finds relevant Q&A from the knowledge base
3. **LLM** generates a contextual reply using topic instructions + RAG results
4. Bot collects required fields â†’ **Escalation** to Zendesk when needed

---

## Quick Start

### Docker (Fastest)

```bash
docker run -d -p 3001:3000 \
  -e GOOGLE_API_KEY=your_key \
  -v qragy-data:/app/data \
  ghcr.io/mahsumaktas/qragy
```

Open `http://localhost:3001` â€” done.

### From Source

```bash
# Clone & install
git clone https://github.com/mahsumaktas/qragy.git
cd qragy && npm install

# Configure (only GOOGLE_API_KEY is required)
cp .env.example .env
# Get a free key at https://aistudio.google.com

# Ingest your knowledge base
node scripts/ingest.js

# Run
npm start
```

Open [localhost:3000](http://localhost:3000) for the chatbot, [localhost:3000/admin](http://localhost:3000/admin) for the admin panel.

---

## Multi-Model Configuration

Qragy supports **Gemini**, **OpenAI**, and **Ollama** (local LLM) out of the box. No extra dependencies needed.

### Gemini (Default, Free Tier)

```env
GOOGLE_API_KEY=your_key_here
```

### OpenAI

```env
LLM_PROVIDER=openai
LLM_API_KEY=sk-...
LLM_MODEL=gpt-4o-mini
EMBEDDING_PROVIDER=openai
EMBEDDING_MODEL=text-embedding-3-small
```

### Ollama (Fully Local, No API Key)

```env
LLM_PROVIDER=ollama
LLM_MODEL=llama3.2
LLM_BASE_URL=http://localhost:11434/v1
EMBEDDING_PROVIDER=ollama
EMBEDDING_MODEL=nomic-embed-text
EMBEDDING_BASE_URL=http://localhost:11434
```

All `GOOGLE_*` environment variables continue to work for backward compatibility. See [`.env.example`](.env.example) for all options.

---

## Deploy

### Render (One-Click)

[![Deploy to Render](https://render.com/images/deploy-to-render-button.svg)](https://render.com/deploy?repo=https://github.com/mahsumaktas/qragy)

### Docker Compose

```bash
git clone https://github.com/mahsumaktas/qragy.git
cd qragy
cp .env.example .env   # add your API key
docker compose up -d
```

### Raspberry Pi

```bash
git clone https://github.com/mahsumaktas/qragy.git
cd qragy && npm install
cp .env.example .env    # add your GOOGLE_API_KEY
node scripts/ingest.js
npm install -g pm2
pm2 start server.js --name qragy
pm2 save && pm2 startup
```

### Any VPS / Docker

Works on any machine with Node.js 18+. No Docker required, but runs fine in a container too.

---

## Configuration

| Variable | Description | Default |
|----------|-------------|---------|
| `GOOGLE_API_KEY` | Gemini API key **(required for Gemini)** | â€” |
| `LLM_PROVIDER` | LLM provider (`gemini`, `openai`, `ollama`) | `gemini` |
| `LLM_API_KEY` | API key (falls back to GOOGLE_API_KEY) | â€” |
| `LLM_MODEL` | Chat model (falls back to GOOGLE_MODEL) | â€” |
| `LLM_BASE_URL` | Custom base URL (Ollama, etc.) | â€” |
| `EMBEDDING_PROVIDER` | Embedding provider | `gemini` |
| `EMBEDDING_MODEL` | Embedding model | `gemini-embedding-001` |
| `GOOGLE_MODEL` | Chat model | `gemini-3-pro-preview` |
| `GOOGLE_FALLBACK_MODEL` | Fallback model on error | â€” |
| `BOT_NAME` | Bot display name | `QRAGY Bot` |
| `COMPANY_NAME` | Your company name | â€” |
| `ADMIN_TOKEN` | Admin panel password | â€” |
| `ZENDESK_ENABLED` | Enable Zendesk handoff | `false` |
| `TELEGRAM_ENABLED` | Enable Telegram bot | `false` |
| `TELEGRAM_BOT_TOKEN` | Telegram Bot API token | â€” |
| `RATE_LIMIT_ENABLED` | Per-IP rate limiting | `true` |
| `RATE_LIMIT_MAX` | Max requests per window | `20` |
| `WEBHOOK_ENABLED` | Enable webhook notifications | `false` |
| `WEBHOOK_URL` | Webhook endpoint URL | â€” |
| `WEBHOOK_SECRET` | HMAC-SHA256 signing secret | â€” |
| `SUPPORT_HOURS_ENABLED` | Enforce business hours | `false` |
| `DETERMINISTIC_COLLECTION_MODE` | Structured info gathering | `true` |

Full list in [`.env.example`](.env.example).

---

## Embedding Widget

Add Qragy to any website:

```html
<script>
  window.__QRAGY_API = "https://your-qragy-server.com";
</script>
<script src="https://your-qragy-server.com/embed.js"></script>
```

---

## Project Structure

```
qragy/
â”œâ”€â”€ server.js                    # API, AI, RAG, tickets â€” single file
â”œâ”€â”€ lib/
â”‚   â”œâ”€â”€ providers.js             # Multi-model LLM + embedding abstraction
â”‚   â””â”€â”€ chunker.js               # Document chunking engine
â”œâ”€â”€ knowledge_base.example.csv   # Example Q&A data
â”œâ”€â”€ agent/                       # Bot personality & rules
â”‚   â”œâ”€â”€ soul.md                  # Identity
â”‚   â”œâ”€â”€ persona.md               # Tone & style
â”‚   â”œâ”€â”€ topics/                  # Structured support flows
â”‚   â”‚   â”œâ”€â”€ _index.json          # Topic registry
â”‚   â”‚   â””â”€â”€ *.md                 # Topic instructions
â”‚   â””â”€â”€ ...                      # Escalation, filtering, etc.
â”œâ”€â”€ memory/                      # Conversation & ticket templates
â”œâ”€â”€ public/                      # Frontend (vanilla JS)
â”‚   â”œâ”€â”€ index.html               # Chat UI
â”‚   â”œâ”€â”€ admin.html               # Admin panel
â”‚   â””â”€â”€ embed.js                 # Embeddable widget
â”œâ”€â”€ scripts/ingest.js            # CSV â†’ LanceDB embedder
â”œâ”€â”€ Dockerfile                   # Docker image definition
â”œâ”€â”€ docker-compose.yml           # Easy container setup
â””â”€â”€ data/                        # Runtime data (auto-created)
```

---

## Qragy vs Alternatives

| | **Qragy** | Dify | Botpress | Intercom |
|---|:---:|:---:|:---:|:---:|
| Fully self-hosted | âœ… | âš ï¸ Partial | âŒ | âŒ |
| Runs on Raspberry Pi | âœ… | âŒ | âŒ | âŒ |
| Multi-model | âœ… | âœ… | âœ… | âŒ |
| Vector DB | Embedded | External | External | Managed |
| Min RAM | **150MB** | 4GB+ | 2GB+ | N/A |
| Monthly cost | **$0** | Free tier limited | Free tier limited | $74+/seat |
| Admin panel | Built-in | âœ… | âœ… | âœ… |
| Setup time | **30 sec** | 30+ min | 15+ min | N/A |
| Open source | MIT | Apache 2.0 | AGPL | âŒ |
| Dependencies | 7 npm packages | Docker + Redis + Postgres | Cloud | Cloud |

---

## API

All admin endpoints require `x-admin-token` header when `ADMIN_TOKEN` is set.

<details>
<summary><strong>View all endpoints</strong></summary>

### Chat
- `POST /api/chat` â€” Send message, get AI response

### Tickets
- `GET /api/admin/summary` â€” Dashboard stats
- `GET /api/admin/tickets` â€” List tickets
- `GET /api/admin/tickets/:id` â€” Ticket detail
- `PUT /api/admin/tickets/:id/assign` â€” Assign to team member
- `PUT /api/admin/tickets/:id/priority` â€” Set priority
- `POST /api/admin/tickets/:id/notes` â€” Add internal note

### Knowledge Base
- `GET /api/admin/knowledge` â€” List entries
- `POST /api/admin/knowledge` â€” Add entry
- `PUT /api/admin/knowledge/:id` â€” Update entry
- `DELETE /api/admin/knowledge/:id` â€” Delete entry
- `POST /api/admin/knowledge/reingest` â€” Rebuild vector index
- `POST /api/admin/knowledge/upload` â€” Upload PDF/DOCX/TXT

### Bot Config
- `GET/PUT /api/admin/agent/files/:name` â€” Read/write agent files
- `GET/POST/PUT/DELETE /api/admin/agent/topics/:id` â€” Topic CRUD
- `GET/PUT /api/admin/agent/memory/:name` â€” Memory templates
- `GET/PUT /api/admin/env` â€” Environment variables

### Analytics & System
- `GET /api/admin/analytics` â€” Metrics and charts
- `GET /api/admin/system` â€” Health info
- `POST /api/admin/agent/reload` â€” Hot-reload config

### Webhooks
- `GET /api/admin/webhooks/config` â€” Get config
- `PUT /api/admin/webhooks/config` â€” Update config
- `POST /api/admin/webhooks/test` â€” Send test webhook

### Prompt Versions
- `GET /api/admin/agent/versions` â€” List versions
- `POST /api/admin/agent/versions/rollback` â€” Rollback

</details>

---

## Tech Stack

| Layer | Technology |
|-------|-----------|
| Runtime | Node.js 18+ |
| Framework | Express.js |
| AI | Gemini, OpenAI, Ollama (multi-provider) |
| Vector DB | LanceDB (embedded, serverless) |
| Embeddings | Gemini / OpenAI / Ollama (configurable) |
| Frontend | Vanilla JS â€” zero build step |
| Storage | CSV + JSON files |
| Container | Docker (optional) |

---

## Contributing

We welcome contributions! See [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.

## License

[MIT](LICENSE) â€” use it however you want.

---

<p align="center">
  <sub>Built with â¤ï¸ by <a href="https://github.com/mahsumaktas">Mahsum Aktas</a></sub>
</p>
